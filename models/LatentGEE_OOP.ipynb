{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9423a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import os\n",
    "import warnings\n",
    "import yaml\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from LatentGEE import(\n",
    "    VAE,\n",
    "    train_vae,\n",
    "    ziln_nll,\n",
    "    evaluate_latentgee_u,\n",
    "    gee_latent_residual,\n",
    "    decode_batch_corrected_latent,\n",
    "    permanova_r2\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Helper 함수\n",
    "# =========================\n",
    "def _safe_silhouette(X: np.ndarray, labels: np.ndarray, metric: str = \"braycurtis\") -> float:\n",
    "    labels = np.asarray(labels)\n",
    "    uniq, counts = np.unique(labels, return_counts=True)\n",
    "    if len(uniq) < 2 or np.any(counts < 2):\n",
    "        return float(\"nan\")\n",
    "    try:\n",
    "        return float(silhouette_score(X, labels, metric=metric))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def _eval_block(X: np.ndarray, labels: np.ndarray, sil_metric: str, r2_metric: str,\n",
    "                permutations: int, standardize: bool) -> Dict[str, float]:\n",
    "    X_in = np.asarray(X, dtype=np.float64)\n",
    "    if standardize:\n",
    "        X_in = StandardScaler().fit_transform(X_in)\n",
    "    sil = _safe_silhouette(X_in, labels, metric=sil_metric)\n",
    "    _, r2 = permanova_r2(X_in, grouping=np.asarray(labels), metric=r2_metric, permutations=permutations)\n",
    "    return {\"silhouette\": float(sil), \"r2\": float(r2)}\n",
    "\n",
    "# ====== 1) YAML 헬퍼 ======    \n",
    "def load_cfg(path: str) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f) or {}\n",
    "  \n",
    "def _as_none(x):\n",
    "    return None if (isinstance(x, str) and x.lower() == \"none\") else x\n",
    "\n",
    "def suggest_auto(trial: optuna.Trial, name: str, spec):\n",
    "    \"\"\"YAML spec을 보고 Optuna의 suggest_*를 자동 선택.\"\"\"\n",
    "    if isinstance(spec, dict):\n",
    "        if \"loguniform\" in spec:\n",
    "            low, high = spec[\"loguniform\"]\n",
    "            return trial.suggest_float(name, float(low), float(high))\n",
    "        raise ValueError(f\"Unsupported dict spec for {name}: {spec}\")\n",
    "    if isinstance(spec, (list, tuple)):\n",
    "        vals = [_as_none(v) for v in spec]\n",
    "        if len(vals) == 2 and all(isinstance(v, numbers.Number) for v in vals):\n",
    "            low, high = vals\n",
    "            if float(low).is_integer() and float(high).is_integer():\n",
    "                return trial.suggest_int(name, int(low), int(high))\n",
    "            else:\n",
    "                return trial.suggest_float(name, float(low), float(high))\n",
    "        return trial.suggest_categorical(name, vals)\n",
    "    return spec  # 단일 고정값\n",
    "\n",
    "# ====== 2) 데이터 로더(Zero prevalence cutoff 캐시 지원) ======\n",
    "# 전역 캐시 (메모리 or 디스크)\n",
    "_DATASET_CACHE = {}\n",
    "\n",
    "def get_dataset_for_cutoff(cutoff: float):\n",
    "    \"\"\"\n",
    "    cutoff별 X_tensor를 CPU 텐서로 반환하고 input_dim을 리턴.\n",
    "    1) 메모리 캐시 있으면 바로 사용\n",
    "    2) 없으면 디스크 캐시(파일) 있나 확인 -> 로드\n",
    "    3) 둘 다 없으면 원시데이터로부터 전처리 수행 -> 캐시/저장\n",
    "    \"\"\"\n",
    "    key = f\"zp_{cutoff:.4f}\"\n",
    "    if key in _DATASET_CACHE:\n",
    "        X = _DATASET_CACHE[key]\n",
    "        return X, X.shape[1]\n",
    "\n",
    "    # (A) 디스크 캐시가 있다면:\n",
    "    pkl_path = f\".../preprocessed/hivrc_scene2_zp{cutoff:.2f}.pt\"\n",
    "    if os.path.exists(pkl_path):\n",
    "        X = torch.load(pkl_path, map_location=\"cpu\")\n",
    "        _DATASET_CACHE[key] = X\n",
    "        return X, X.shape[1]\n",
    "\n",
    "    # (B) 원시데이터로부터 전처리 (여기서는 의사코드)\n",
    "    # raw = load_raw(...)\n",
    "    # X_np = preprocess_by_zero_prevalence(raw, cutoff=cutoff)  # np.ndarray (N,D)\n",
    "    # X = torch.tensor(X_np, dtype=torch.float32)\n",
    "    # torch.save(X, pkl_path)\n",
    "    # _DATASET_CACHE[key] = X\n",
    "    # return X, X.shape[1]\n",
    "    raise FileNotFoundError(f\"no dataset for cutoff={cutoff}; add preprocessing or cached file.\")\n",
    "\n",
    "# =========================\n",
    "# Configs\n",
    "# =========================\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    zero_prevalence_cutoff: list[float]\n",
    "    standardize_latent: list[bool]\n",
    "    tensor_path: str\n",
    "\n",
    "# 모델 탐색 공간\n",
    "@dataclass\n",
    "class ModelSearchSpace:\n",
    "    n_layers: list[int]\n",
    "    latent_dim: list[int]\n",
    "    base_dim: list[int]\n",
    "    strategy: list[str]\n",
    "    dropout_rate: list[float]\n",
    "    beta_kl: list[float]\n",
    "    kl_warmup_epochs: list[int]\n",
    "    norm: list[str]\n",
    "    init: list[str]\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    epochs: int = 50\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 256\n",
    "    num_workers: int = max(2, (os.cpu_count() or 4) - 2)\n",
    "    amp: bool = field(default_factory=lambda: torch.cuda.is_available())\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    input_dim: int\n",
    "    latent_dim: int = 16\n",
    "    n_layers: int = 2\n",
    "    base_dim: int = 128\n",
    "    strategy: str = \"constant\"   # {'constant','halve','double'}\n",
    "    dropout: float = 0.0\n",
    "    activation: str = \"relu\"\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    # HDBSCAN pseudo-batch\n",
    "    hdb_min_cluster_size: int = 10\n",
    "    hdb_min_samples: Optional[int] = None\n",
    "    hdb_metric: str = \"braycurtis\"  # e.g. 'euclidean','cosine','braycurtis' (하이픈 X)\n",
    "    allow_noise: bool = True\n",
    "    # PERMANOVA\n",
    "    permanova_metric: str = \"braycurtis\"\n",
    "    permanova_permutations: int = 999\n",
    "\n",
    "@dataclass\n",
    "class TuningConfig:\n",
    "    n_trials: int = 25\n",
    "    timeout: Optional[int] = None\n",
    "    pruner: bool = True\n",
    "\n",
    "# =========================\n",
    "# 학습 탐색 공간\n",
    "# =========================\n",
    "@dataclass\n",
    "class TrainingSearchSpace:\n",
    "    epochs: list[int]\n",
    "    batch_size: list[int]\n",
    "    learning_rate: dict\n",
    "    weight_decay: dict\n",
    "    optimizer: list[str]\n",
    "    amp: list[bool]\n",
    "    grad_clip_norm: list[float]\n",
    "    scheduler: list[str]\n",
    "    warmup_epochs: list[int]\n",
    "\n",
    "# 클러스터링 탐색 공간\n",
    "@dataclass\n",
    "class ClusteringSearchSpace:\n",
    "    min_cluster_size: list[int]\n",
    "    min_samples: list[Any]     # \"None\"이 문자열로 들어오므로 Any\n",
    "    metric: list[str]\n",
    "    cluster_selection_method: list[str]\n",
    "    allow_single_cluster: list[bool]\n",
    "\n",
    "# 평가 옵션\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    silhouette_metric: list[str]\n",
    "    noise_handling: list[str]\n",
    "\n",
    "# Safeguards\n",
    "@dataclass\n",
    "class SafeguardsConfig:\n",
    "    eps: float\n",
    "    max_oom_retries: int\n",
    "\n",
    "# 전체 config wrapper\n",
    "@dataclass\n",
    "class FullConfig:\n",
    "    data: DataConfig\n",
    "    search_space: dict\n",
    "    safeguards: SafeguardsConfig\n",
    "\n",
    "# =========================\n",
    "# Data Module\n",
    "# =========================\n",
    "class DataModule:\n",
    "    def __init__(self, X: np.ndarray, train_cfg: TrainConfig):\n",
    "        assert isinstance(X, np.ndarray), \"X must be a numpy array\"\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.train_cfg = train_cfg\n",
    "\n",
    "    def make_loader(self) -> DataLoader:\n",
    "        ds = TensorDataset(self.X.cpu())\n",
    "        return DataLoader(\n",
    "            ds,\n",
    "            batch_size=self.train_cfg.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.train_cfg.num_workers,\n",
    "            prefetch_factor=4 if self.train_cfg.num_workers > 0 else None,\n",
    "            persistent_workers=(self.train_cfg.num_workers > 0),\n",
    "            pin_memory=(self.train_cfg.device == \"cuda\"),\n",
    "            drop_last=True,\n",
    "        )\n",
    "# =========================\n",
    "# LatentGEEModule (Model + Train loop)\n",
    "# =========================\n",
    "class LatentGEEModule(nn.Module):\n",
    "    def __init__(self, model_cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        # VAE 객체 생성 (LatentGEE.py에 있는 VAE 클래스) \n",
    "        self.vae = VAE(\n",
    "            input_dim=model_cfg.input_dim,\n",
    "            latent_dim=model_cfg.latent_dim,\n",
    "            n_layers=model_cfg.n_layers,\n",
    "            base_dim=model_cfg.base_dim,\n",
    "            strategy=model_cfg.strategy,\n",
    "            dropout_rate=model_cfg.dropout,\n",
    "            activation=model_cfg.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        VAE forward 호출 (encode -> reparameterize -> decode)\n",
    "        \"\"\"\n",
    "        return self.vae(x)\n",
    "\n",
    "    def fit(self, loader: DataLoader, train_cfg: TrainConfig) -> float:\n",
    "        \"\"\"\n",
    "        주어진 DataLoader로 VAE 학습 수행 (ZILN NLL + KL 손실).\n",
    "        마지막 배치의 loss(float)를 반환.\n",
    "        \"\"\"\n",
    "        device = train_cfg.device\n",
    "        self.to(device)\n",
    "        self.train()\n",
    "\n",
    "        opt = torch.optim.Adam(self.parameters(), lr=train_cfg.lr)\n",
    "        use_amp = train_cfg.amp and (device == \"cuda\")\n",
    "        scaler = GradScaler(device=\"cuda\", enabled=use_amp)\n",
    "\n",
    "        last_loss = float(\"inf\")\n",
    "\n",
    "        for ep in range(train_cfg.epochs):\n",
    "            for (xb,) in loader:\n",
    "                xb = xb.to(device, non_blocking=use_amp)\n",
    "                with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    (pi, mu_x, logsigma_x), mu_z, logvar_z, _ = self.vae(xb)\n",
    "                    recon_nll = ziln_nll(xb, pi, mu_x, logsigma_x)\n",
    "                    kl = -0.5 * torch.mean(1 + logvar_z - mu_z.pow(2) - logvar_z.exp())\n",
    "                    loss = recon_nll + kl\n",
    "\n",
    "                if torch.isnan(loss):\n",
    "                    return float(\"inf\")\n",
    "\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "\n",
    "                last_loss = float(loss.detach().cpu().item())\n",
    "\n",
    "        return last_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_mu(self, X_tensor: torch.Tensor, device: Optional[str] = None) -> torch.Tensor:\n",
    "        \"\"\"입력 X를 μ(latent mean)으로 인코딩.\"\"\"\n",
    "        self.eval()\n",
    "        dev = device or next(self.parameters()).device\n",
    "        mu, _ = self.vae.encode(X_tensor.to(dev, non_blocking=(dev.type == \"cuda\")))\n",
    "        return mu.detach().cpu()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_from_latent(self, z: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"latent z에서 X를 디코딩 (ZILN 기대값으로 복원).\"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        z_tensor = torch.tensor(z, dtype=torch.float32, device=device)\n",
    "        pi, mu_x, log_sigma_x = self.vae.decode(z_tensor)\n",
    "        sigma = log_sigma_x.exp()\n",
    "        x_hat = (1 - pi) * torch.exp(mu_x + 0.5 * sigma * sigma)\n",
    "        return x_hat.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Evaluator (silhouette / PERMANOVA R²)\n",
    "# -----------------------\n",
    "class BatchEffectEvaluator:\n",
    "    def __init__(self, eval_cfg: EvalConfig):\n",
    "        self.cfg = eval_cfg\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def silhouette_from_model(self, model: LatentGEEModule, X: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        labels, sil = evaluate_latentgee_u(\n",
    "            model=model.vae,\n",
    "            X_tensor=X_tensor,\n",
    "            min_cluster_size=self.cfg.hdb_min_cluster_size,\n",
    "            min_samples=self.cfg.hdb_min_samples,\n",
    "            allow_noise=self.cfg.allow_noise,\n",
    "            metric=self.cfg.hdb_metric,\n",
    "        )\n",
    "        return labels, float(sil)\n",
    "\n",
    "    def permanova_r2_from_matrix(self, X: np.ndarray, labels: np.ndarray) -> Tuple[Any, float]:\n",
    "        X_std = StandardScaler().fit_transform(X)\n",
    "        res, r2 = permanova_r2(\n",
    "            X_std,\n",
    "            grouping=labels,\n",
    "            metric=self.cfg.permanova_metric,\n",
    "            permutations=self.cfg.permanova_permutations,\n",
    "        )\n",
    "        return res, float(r2)\n",
    "    \n",
    "# -----------------------\n",
    "# Corrector (latent GEE residual → decode)\n",
    "# -----------------------\n",
    "class BatchCorrector:\n",
    "    @torch.no_grad()\n",
    "    def correct_and_decode(\n",
    "        self,\n",
    "        model: LatentGEEModule,\n",
    "        X: np.ndarray,\n",
    "        labels: np.ndarray,                    # ← 반드시 외부에서 라벨을 받아옴 (pseudo-batch or real batch)\n",
    "        save_path: Optional[str] = None\n",
    "    ) -> np.ndarray:\n",
    "        # 1) encode μ\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        mu = model.encode_mu(X_tensor)                        # torch.Tensor\n",
    "        mu_np = mu.detach().cpu().numpy().astype(\"float32\")   # np.ndarray (N,k)\n",
    "\n",
    "        # 2) residualize (labels는 np.ndarray)\n",
    "        z_tilde = gee_latent_residual(mu_np, labels)          # np.ndarray (N,k)\n",
    "\n",
    "        # 3) decode\n",
    "        x_corr = model.decode_from_latent(z_tilde)            # np.ndarray (N,D)\n",
    "        if save_path:\n",
    "            np.save(save_path, x_corr)\n",
    "        return x_corr\n",
    "# -----------------------\n",
    "# Optuna Tuner\n",
    "# -----------------------\n",
    "class OptunaTuner:\n",
    "    def __init__(self, X: np.ndarray, base_model_cfg: ModelConfig, train_cfg: TrainConfig, eval_cfg: EvalConfig):\n",
    "        self.X = X\n",
    "        self.base_model_cfg = base_model_cfg\n",
    "        self.train_cfg = train_cfg\n",
    "        self.eval_cfg = eval_cfg\n",
    "\n",
    "    def _suggest(self, trial: optuna.Trial) -> ModelConfig:\n",
    "        # 예시: latent_dim, base_dim, n_layers, dropout, activation 탐색\n",
    "        latent_dim = trial.suggest_int(\"latent_dim\", 8, 64, step=8)\n",
    "        base_dim   = trial.suggest_categorical(\"base_dim\", [64, 128, 256, 512])\n",
    "        n_layers   = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "        dropout    = trial.suggest_float(\"dropout\", 0.0, 0.4, step=0.1)\n",
    "        activation = trial.suggest_categorical(\"activation\", [\"relu\", \"leaky_relu\", \"gelu\"])\n",
    "        strategy   = trial.suggest_categorical(\"strategy\", [\"constant\", \"halve\", \"double\"])\n",
    "        return ModelConfig(\n",
    "            input_dim=self.base_model_cfg.input_dim,\n",
    "            latent_dim=latent_dim,\n",
    "            n_layers=n_layers,\n",
    "            base_dim=base_dim,\n",
    "            strategy=strategy,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "        )\n",
    "    \n",
    "    def objective(self, trial: optuna.Trial) -> float:\n",
    "        # 단일 목표: pseudo-batch silhouette (↓)\n",
    "        model_cfg = self._suggest(trial)\n",
    "        dm = DataModule(self.X, self.train_cfg)\n",
    "        loader = dm.make_loader()\n",
    "\n",
    "        model = LatentGEEModule(model_cfg)\n",
    "        _ = model.fit(loader, self.train_cfg)\n",
    "\n",
    "        evaluator = BatchEffectEvaluator(self.eval_cfg)\n",
    "        _, sil = evaluator.silhouette_from_model(model, self.X)\n",
    "\n",
    "        trial.report(float(sil), step=0)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "        return float(sil)\n",
    "\n",
    "    def tune(self, tune_cfg: TuningConfig) -> Tuple[optuna.Study, Dict[str, Any]]:\n",
    "        pruner = optuna.pruners.MedianPruner() if tune_cfg.pruner else optuna.pruners.NopPruner()\n",
    "        study = optuna.create_study(direction=\"minimize\", pruner=pruner)  # silhouette ↓\n",
    "        study.optimize(self.objective, n_trials=tune_cfg.n_trials, timeout=tune_cfg.timeout)\n",
    "        return study, study.best_params\n",
    "\"\"\"\n",
    "    def objective_multi(self, trial: optuna.Trial, y_bio: np.ndarray) -> Tuple[float, float, float, float]:\n",
    "        \"\"\"\n",
    "        Multi-objective: (sil_batch ↓, r2_batch ↓, sil_bio ↑, r2_bio ↑)\n",
    "        \"\"\"\n",
    "        model_cfg = self._suggest(trial)\n",
    "        dm = DataModule(self.X, self.train_cfg)\n",
    "        loader = dm.make_loader()\n",
    "\n",
    "        model = LatentGEEModule(model_cfg)\n",
    "        _ = model.fit(loader, self.train_cfg)\n",
    "\n",
    "        # batch metrics\n",
    "        evaluator = BatchEffectEvaluator(self.eval_cfg)\n",
    "        labels_batch, sil_batch = evaluator.silhouette_from_model(model, self.X)\n",
    "\n",
    "        X_std = StandardScaler().fit_transform(self.X)\n",
    "        _, r2_batch = permanova_r2(\n",
    "            X_std, grouping=labels_batch,\n",
    "            metric=self.eval_cfg.permanova_metric,\n",
    "            permutations=self.eval_cfg.permanova_permutations,\n",
    "        )\n",
    "\n",
    "        # bio metrics (encoder μ 기준)\n",
    "        with torch.no_grad():\n",
    "            mu = model.encode_mu(torch.tensor(self.X, dtype=torch.float32, device=self.train_cfg.device))\n",
    "        Z = mu.detach().cpu().numpy()\n",
    "        Z_std = StandardScaler().fit_transform(Z)\n",
    "\n",
    "        sil_bio = silhouette_score(Z_std, y_bio, metric=\"euclidean\")\n",
    "        _, r2_bio = permanova_r2(\n",
    "            Z_std, grouping=y_bio,\n",
    "            metric=self.eval_cfg.permanova_metric,\n",
    "            permutations=self.eval_cfg.permanova_permutations,\n",
    "        )\n",
    "\n",
    "        return float(sil_batch), float(r2_batch), float(sil_bio), float(r2_bio)\n",
    "\"\"\"\n",
    "def objective_multi(trial: \"optuna.Trial\"):\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Pipeline\n",
    "# =========================\n",
    "class LatentGEEPipeline:\n",
    "    def __init__(self, X: np.ndarray, model_cfg: ModelConfig, train_cfg: TrainConfig, eval_cfg: EvalConfig):\n",
    "        self.X = X\n",
    "        self.model_cfg = model_cfg\n",
    "        self.train_cfg = train_cfg\n",
    "        self.eval_cfg = eval_cfg\n",
    "        self.model: Optional[LatentGEEModule] = None\n",
    "\n",
    "    def fit(self):\n",
    "        print(\"[LatentGEEPipeline] Starting: preparing model training\")\n",
    "        print(f\"  - Data size: {self.X.shape}, batch_size={self.train_cfg.batch_size}, epochs={self.train_cfg.epochs}\")\n",
    "        print(\"[LatentGEEPipeline] Training started...\")\n",
    "        \n",
    "        # DataModule 시그니처: (X, train_cfg)\n",
    "        dm = DataModule(self.X, self.train_cfg)\n",
    "        loader = dm.make_loader()\n",
    "        self.model = LatentGEEModule(self.model_cfg)\n",
    "        self.model.fit(loader, self.train_cfg)\n",
    "        print(\"[LatentGEEPipeline] Training finished\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_mu(self) -> np.ndarray:\n",
    "        \"\"\"학습된 모델로 X를 μ(latent mean)로 인코딩해서 numpy(float32)로 반환.\"\"\"\n",
    "\n",
    "        print(\"[LatentGEEPipeline] Encoding: computing latent mean (μ)\")\n",
    "        \n",
    "        assert self.model is not None, \"Call fit() first.\"\n",
    "        self.model.eval()\n",
    "        X_tensor = torch.tensor(self.X, dtype=torch.float32)\n",
    "        mu = self.model.encode_mu(X_tensor)                # torch.Tensor on CPU (우리 구현 기준)\n",
    "        mu_np = mu.detach().cpu().numpy().astype(\"float32\")\n",
    "        print(f\"[LatentGEEPipeline] Encoding finished: shape={mu_np.shape}\")\n",
    "        return mu_np\n",
    "\n",
    "    def evaluate(self) -> Dict[str, Any]:\n",
    "        \"\"\"배치 지표 평가 (pseudo-batch silhouette & PERMANOVA R²).\"\"\"\n",
    "        print(\"[LatentGEEPipeline] Evaluation started: pseudo-batch silhouette & PERMANOVA R²\")\n",
    "\n",
    "        assert self.model is not None, \"Call fit() first.\"\n",
    "        evaluator = BatchEffectEvaluator(self.eval_cfg)\n",
    "        labels, sil = evaluator.silhouette_from_model(self.model, self.X)\n",
    "        res, r2 = evaluator.permanova_r2_from_matrix(self.X, labels)\n",
    "        print(f\"[LatentGEEPipeline] Evaluation finished: silhouette={sil:.4f}, R²={r2:.4f}\")\n",
    "        return {\"silhouette_batch\": sil, \"permanova_r2_batch\": r2, \"labels\": labels, \"permanova\": res}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def correct_and_decode(self, save_path: Optional[str] = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        1) μ 인코딩 → 2) pseudo-batch 라벨 추정 → 3) latent residualize → 4) decode → 5) (선택) 저장\n",
    "        \"\"\"\n",
    "        print(\"[LatentGEEPipeline] Correction + decoding started\")\n",
    "        print(\"  1) Encoding latent mean (μ)...\")\n",
    "        assert self.model is not None, \"Call fit() first.\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # 1) μ 인코딩\n",
    "        X_tensor = torch.tensor(self.X, dtype=torch.float32)\n",
    "        mu = self.model.encode_mu(X_tensor)                # torch.Tensor on CPU\n",
    "        mu_np = mu.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "        # 2) 라벨 추정 (evaluate_latentgee_u는 model.vae를 받도록 설계됨)\n",
    "        print(\"  2) Estimating pseudo-batch labels...\")        \n",
    "        labels, _ = evaluate_latentgee_u(\n",
    "            model=self.model.vae,\n",
    "            X_tensor=X_tensor,\n",
    "            min_cluster_size=self.eval_cfg.hdb_min_cluster_size,\n",
    "            min_samples=self.eval_cfg.hdb_min_samples,\n",
    "            allow_noise=True,\n",
    "            metric=self.eval_cfg.hdb_metric,\n",
    "        )\n",
    "\n",
    "        # 3) residualize\n",
    "        print(\"  3) Running GEE residualization (removing batch effect)...\")\n",
    "        z_tilde = gee_latent_residual(mu_np, labels)       # np.ndarray (N,k), LatentGEE.py 시그니처 기준\n",
    "\n",
    "        # 4) decode\n",
    "        print(\"  4) Decoding corrected latent back to data space...\")\n",
    "        x_corr = self.model.decode_from_latent(z_tilde)    # np.ndarray (N,D)\n",
    "\n",
    "        # 5) 저장(옵션)\n",
    "        if save_path:\n",
    "            np.save(save_path, x_corr)\n",
    "            print(f\"[LatentGEEPipeline] Corrected data saved to {save_path}\")\n",
    "\n",
    "        return x_corr\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def correct_decode_and_evaluate_strict(\n",
    "        self,                                   # LatentGEEPipeline 인스턴스로 바인딩해서 사용\n",
    "        *,\n",
    "        original_batch_labels: np.ndarray,       # 입력 데이터의 \"실제\" 배치 라벨 (필수, N,)\n",
    "        original_bio_labels:   np.ndarray,       # 입력 데이터의 \"실제\" 바이오/임상 라벨 (필수, N,)\n",
    "        silhouette_metric: str = \"braycurtis\",\n",
    "        r2_metric: str = \"braycurtis\",\n",
    "        permutations: int = 999,\n",
    "        standardize: bool = True,\n",
    "        save_path: Optional[str] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        흐름:\n",
    "        1) 원본 X에서 pseudo-batch 라벨 생성 (HDBSCAN→GEE-residual→silhouette 유틸 재사용; 항상 수행)\n",
    "        2) before_batch: 원본 X에서 pseudo-batch 라벨 기준 평가\n",
    "        3) X를 보정: pseudo-batch를 그룹 변수로 GEE 적합, y_hat에서 residual을 빼 batch effect 제거(= gee_latent_residual)\n",
    "        4) 디코드 → X_corr\n",
    "        5) after_batch: X_corr에서 input데이터의 original batch 라벨 기준 평가\n",
    "        6) after_bio  : X_corr에서 input데이터의 original bio 라벨 기준 평가\n",
    "\n",
    "        반환:\n",
    "        {\n",
    "            'before_batch': {...},   # (pseudo-batch 기준) ↓가 좋음\n",
    "            'after_batch':  {...},   # (original batch 기준) ↓가 좋음\n",
    "            'after_bio':    {...},   # (original bio 기준)   ↑가 좋음\n",
    "            'X_corr':       np.ndarray\n",
    "        }\n",
    "        \"\"\"\n",
    "        print(\"[LatentGEEPipeline] Strict evaluation started\")\n",
    "        assert self.model is not None, \"Call fit() first.\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # 1) pseudo-batch 생성 (항상 수행)\n",
    "        print(\"  1) Generating pseudo-batch (HDBSCAN)...\")\n",
    "        X_tensor = torch.tensor(self.X, dtype=torch.float32)\n",
    "        pseudo_labels, _ = evaluate_latentgee_u(\n",
    "            model=self.model.vae,   # 주의: wrapper가 아닌 내부 VAE를 넘김\n",
    "            X_tensor=X_tensor,\n",
    "            min_cluster_size=self.eval_cfg.hdb_min_cluster_size,\n",
    "            min_samples=self.eval_cfg.hdb_min_samples,\n",
    "            allow_noise=self.eval_cfg.allow_noise,\n",
    "            metric=self.eval_cfg.hdb_metric,\n",
    "        )\n",
    "\n",
    "        # 2) BEFORE (원본 X, pseudo-batch 기준 평가)\n",
    "        print(\"  2) BEFORE evaluation (pseudo-batch based)...\")\n",
    "        before_batch = _eval_block(\n",
    "            self.X, pseudo_labels,\n",
    "            sil_metric=silhouette_metric, r2_metric=r2_metric,\n",
    "            permutations=permutations, standardize=standardize\n",
    "        )\n",
    "\n",
    "        # 3) 보정(잠재공간): pseudo-batch를 그룹으로 GEE 적합 → batch 효과 제거\n",
    "        #   - 구현은 LatentGEE.py의 gee_latent_residual이 수행(= y_hat - residual과 동치인 batch 효과 제거 결과)\n",
    "        print(\"  3) Performing correction (latent residualization)...\")\n",
    "        mu = self.model.encode_mu(X_tensor)                              # torch.Tensor (N,k)\n",
    "        mu_np = mu.detach().cpu().numpy().astype(\"float32\")              # np.ndarray (N,k)\n",
    "        z_tilde = gee_latent_residual(mu_np, pseudo_labels)              # np.ndarray (N,k), batch-corrected latent\n",
    "\n",
    "        # 4) 디코드 → X_corr\n",
    "        print(\"  4) Decoding corrected latent to data space...\")\n",
    "        X_corr = self.model.decode_from_latent(z_tilde)                  # np.ndarray (N,D)\n",
    "        if save_path:\n",
    "            np.save(save_path, X_corr)\n",
    "\n",
    "        # 5) AFTER: 보정된 X에서 \"original batch\" 기준 평가\n",
    "        print(\"  5) AFTER evaluation (original batch labels)...\")\n",
    "        after_batch = _eval_block(\n",
    "            X_corr, np.asarray(original_batch_labels),\n",
    "            sil_metric=silhouette_metric, r2_metric=r2_metric,\n",
    "            permutations=permutations, standardize=standardize\n",
    "        )\n",
    "\n",
    "        # 6) AFTER: 보정된 X에서 \"original bio\" 기준 평가\n",
    "        print(\"  6) AFTER evaluation (original bio labels)...\")\n",
    "        after_bio = _eval_block(\n",
    "            X_corr, np.asarray(original_bio_labels),\n",
    "            sil_metric=silhouette_metric, r2_metric=r2_metric,\n",
    "            permutations=permutations, standardize=standardize\n",
    "        )\n",
    "        print(\"[LatentGEEPipeline] Strict evaluation finished\")\n",
    "        return {\n",
    "            \"before_batch\": before_batch,\n",
    "            \"after_batch\":  after_batch,\n",
    "            \"after_bio\":    after_bio,\n",
    "            \"X_corr\":       X_corr,\n",
    "        }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helper: retrain → residualize → decode → save\n",
    "# Optuna 결과를 최종 적용해서 “공식 corrected 데이터셋” 을 뽑을 때 필요한 함수\n",
    "# 그냥 파이프라인 내부에서 학습하고 바로 평가만 한다면 → 불필요 (중복 기능).\n",
    "# 최종 논문/실험에서 “best_params로 retrain 후 결과 저장” 프로세스를 쓸 거라면 그대로 두는 게 맞음\n",
    "# =========================\n",
    "def retrain_encode_residual_decode_save(\n",
    "    X: np.ndarray,\n",
    "    best_params: Dict[str, Any],\n",
    "    eval_cfg: EvalConfig,\n",
    "    save_path: str = \"X_corrected.npy\",\n",
    "):\n",
    "    # 1) 모델 구성\n",
    "    model = VAE(\n",
    "        input_dim=X.shape[1],\n",
    "        latent_dim=best_params.get(\"latent_dim\", 16),\n",
    "        n_layers=best_params.get(\"n_layers\", 2),\n",
    "        base_dim=best_params.get(\"base_dim\", 128),\n",
    "        strategy=best_params.get(\"strategy\", \"constant\"),\n",
    "        dropout_rate=best_params.get(\"dropout\", 0.0),\n",
    "        activation=best_params.get(\"activation\", \"relu\"),\n",
    "    )\n",
    "\n",
    "    # 2) 학습 (ZILN NLL + KL)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    _ = train_vae(\n",
    "        model,\n",
    "        torch.tensor(X, dtype=torch.float32),\n",
    "        epochs=best_params.get(\"epochs\", 50),\n",
    "        lr=best_params.get(\"lr\", 1e-3),\n",
    "        device=device,\n",
    "        batch_size=best_params.get(\"batch_size\", 256),\n",
    "    )\n",
    "\n",
    "    # 3) μ 인코딩\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mu, _ = model.encode(torch.tensor(X, dtype=torch.float32, device=next(model.parameters()).device))\n",
    "    mu_np = mu.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "    # 4) 라벨 추정 → residualize\n",
    "    labels, _ = evaluate_latentgee_u(\n",
    "        model=model,\n",
    "        X_tensor=torch.tensor(X, dtype=torch.float32),\n",
    "        min_cluster_size=eval_cfg.hdb_min_cluster_size,\n",
    "        min_samples=eval_cfg.hdb_min_samples,\n",
    "        allow_noise=True,\n",
    "        metric=eval_cfg.hdb_metric,\n",
    "    )\n",
    "    z_tilde = gee_latent_residual(mu_np, labels)  # np.ndarray (N,k)\n",
    "\n",
    "    # 5) 디코드 + 저장\n",
    "    x_corrected = decode_batch_corrected_latent(model, z_tilde, save_path=save_path)\n",
    "    return x_corrected, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latentgee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
